# Description
CLIP (https://openai.com/index/clip/) is a strong foundation model, that can be naturally used for zero-shot image classification. A pivotal result (https://arxiv.org/abs/2103.00020) is that accuracy of CLIP-based zero-shot classification is comparable to the SotA accuracy on ImageNet.
With some simple tricks CLIP can also be used for zero-shot object detection (https://www.pinecone.io/learn/series/image-search/zero-shot-object-detection-clip/). However, in contrast to classification, the quality of CLIP-based zero-shot object detection is far from SotA.
The goal of this project is to evaluate the quality of CLIP-based zero-shot object detection and develop some techniques that can help to improve it.
# Roadmap
1. Start from playing with code from https://www.pinecone.io/learn/series/image-search/zero-shot-object-detection-clip/.
2. Choose a dataset for objection detection.
3. Evaluate the CLIP-based zero-shot object detection on this dataset.
4. Discuss some ideas how to improve it with course instructor.