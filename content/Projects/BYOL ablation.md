# Description
Some modern SSL models, e.g. BYOL (https://arxiv.org/abs/2006.07733), SimSiam (https://arxiv.org/abs/2011.10566), etc., are trained via so-called *self-distillation* mechanism and show strong empirical results. However, there is no solid theoretical understanding why these methods do not collapse.
The goal of this project is to carry out an empirical study to find out which components of these methods are crucial and which are not important for their performance.
# Roadmap
1. Read the BYOL paper.
2. Reproduce the experiments from the [blogpost](https://imbue.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/)
3. Discuss how to proceed with course instructor.